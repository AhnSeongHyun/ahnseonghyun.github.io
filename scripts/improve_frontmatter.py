#!/usr/bin/env python3
"""
Frontmatter Auto-Improvement Script

Automatically improves frontmatter quality in markdown blog posts:
1. Auto-generates descriptions from first 200 chars of content
2. Removes markdown/HTML syntax from descriptions
3. Ensures minimum 50 character descriptions
4. Adds missing tags
5. Always creates backup files (filename_bak.md)
"""

import os
import re
import shutil
from pathlib import Path
from typing import Tuple, Optional, Dict, List
import yaml


class FrontmatterImprover:
    def __init__(self, dry_run=False):
        self.dry_run = dry_run
        self.min_description_length = 50
        self.auto_description_length = 200

    def parse_frontmatter(self, content: str) -> Tuple[Optional[Dict], str, Optional[str]]:
        """
        Parse frontmatter and body

        Returns:
            (frontmatter_dict, body, raw_frontmatter_text) tuple
        """
        pattern = r'^---\s*\n(.*?)\n---\s*\n(.*)$'
        match = re.match(pattern, content, re.DOTALL)

        if match:
            raw_frontmatter = match.group(1)
            body = match.group(2)
            try:
                frontmatter = yaml.safe_load(raw_frontmatter)
                return frontmatter, body, raw_frontmatter
            except yaml.YAMLError as e:
                print(f"  âš ï¸  YAML íŒŒì‹± ì˜¤ë¥˜: {e}")
                return None, content, None
        else:
            return None, content, None

    def clean_text_for_description(self, text: str) -> str:
        """Remove markdown/HTML and clean text for description"""
        # Remove markdown headers
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove markdown bold/italic
        text = re.sub(r'\*\*(.+?)\*\*', r'\1', text)
        text = re.sub(r'\*(.+?)\*', r'\1', text)
        text = re.sub(r'__(.+?)__', r'\1', text)
        text = re.sub(r'_(.+?)_', r'\1', text)

        # Remove HTML tags
        text = re.sub(r'<[^>]+>', '', text)

        # Remove markdown links [text](url)
        text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)

        # Remove markdown images
        text = re.sub(r'!\[([^\]]*)\]\([^\)]+\)', r'\1', text)

        # Remove code blocks
        text = re.sub(r'```[\s\S]*?```', '', text)
        text = re.sub(r'`([^`]+)`', r'\1', text)

        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()

        return text

    def generate_description(self, body: str, current_description: str = '') -> str:
        """Generate description from body content"""
        # Clean current description if it exists and is decent
        if current_description:
            cleaned = self.clean_text_for_description(current_description)
            if len(cleaned) >= self.min_description_length:
                return cleaned

        # Generate from body
        cleaned_body = self.clean_text_for_description(body)

        # Take first N characters
        if len(cleaned_body) >= self.auto_description_length:
            description = cleaned_body[:self.auto_description_length].rsplit(' ', 1)[0] + '...'
        else:
            description = cleaned_body

        return description

    def extract_tags_from_content(self, title: str, body: str) -> List[str]:
        """Extract potential tags from title and body"""
        tags = []

        # Common tech keywords to look for
        tech_keywords = [
            'python', 'javascript', 'typescript', 'react', 'vue', 'node',
            'django', 'flask', 'fastapi', 'docker', 'kubernetes', 'aws',
            'gcp', 'azure', 'git', 'github', 'ci/cd', 'testing', 'tdd',
            'api', 'rest', 'graphql', 'database', 'sql', 'nosql', 'mongodb',
            'postgresql', 'redis', 'nginx', 'linux', 'devops', 'agile',
            'scrum', 'retrospective', 'cto', 'leadership', 'startup',
            'mvp', 'product', 'engineering', 'architecture', 'design'
        ]

        content_lower = (title + ' ' + body).lower()

        for keyword in tech_keywords:
            if keyword in content_lower:
                tags.append(keyword)
                if len(tags) >= 5:  # Limit to 5 auto-generated tags
                    break

        return tags

    def backup_file(self, filepath: Path) -> Path:
        """Create backup file"""
        backup_path = filepath.parent / f"{filepath.stem}_bak.md"

        if backup_path.exists():
            # Backup already exists, don't overwrite
            return backup_path

        shutil.copy2(filepath, backup_path)
        print(f"  âœ… ë°±ì—… ìƒì„±: {backup_path.name}")
        return backup_path

    def improve_frontmatter(self, filepath: Path) -> bool:
        """
        Improve frontmatter in a single file

        Returns:
            True if changes were made, False otherwise
        """
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()

            frontmatter, body, raw_frontmatter = self.parse_frontmatter(content)

            if frontmatter is None:
                print(f"â­ï¸  ê±´ë„ˆë›°ê¸°: {filepath.name} (Frontmatter ì—†ìŒ)")
                return False

            changes = []
            modified = False

            # Check and improve description
            current_desc = frontmatter.get('description', '')
            if not current_desc or len(current_desc) < self.min_description_length or '<' in current_desc or '**' in current_desc:
                new_desc = self.generate_description(body, current_desc)
                if new_desc != current_desc:
                    frontmatter['description'] = new_desc
                    modified = True
                    changes.append(f"ì„¤ëª… ê°œì„  ({len(current_desc)} â†’ {len(new_desc)} chars)")

            # Check and add tags
            current_tags = frontmatter.get('tags', [])
            if isinstance(current_tags, str):
                current_tags = [current_tags]

            if len(current_tags) <= 1:
                title = frontmatter.get('title', '')
                auto_tags = self.extract_tags_from_content(title, body)

                # Merge with existing tags
                all_tags = list(set(current_tags + auto_tags))

                if len(all_tags) > len(current_tags):
                    frontmatter['tags'] = all_tags
                    modified = True
                    changes.append(f"íƒœê·¸ ì¶”ê°€ ({len(current_tags)} â†’ {len(all_tags)})")

            if not modified:
                return False

            print(f"\nğŸ“ ê°œì„  ì¤‘: {filepath.name}")
            for change in changes:
                print(f"  â€¢ {change}")

            if self.dry_run:
                print(f"  ğŸ” [DRY RUN] ë³€ê²½ì‚¬í•­ ë¯¸ì €ì¥")
                return True

            # Backup original file
            self.backup_file(filepath)

            # Reconstruct file with improved frontmatter
            new_frontmatter_yaml = yaml.dump(frontmatter,
                                            allow_unicode=True,
                                            default_flow_style=False,
                                            sort_keys=False)

            new_content = f"---\n{new_frontmatter_yaml}---\n\n{body}"

            # Save improved version
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(new_content)

            print(f"  âœ… ì €ì¥ ì™„ë£Œ")
            return True

        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜: {str(e)}")
            return False

    def improve_directory(self, directory: Path, year_filter: int = 2020,
                         max_files: Optional[int] = None):
        """
        Improve frontmatter in all markdown files in directory

        Args:
            directory: Target directory
            year_filter: Only process files from this year onwards
            max_files: Maximum number of files to process
        """
        md_files = list(directory.glob('**/*.md'))

        # Exclude _bak.md files
        md_files = [f for f in md_files if not f.stem.endswith('_bak')]

        # Filter by year if pub_date is available
        if year_filter:
            filtered_files = []
            for filepath in md_files:
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        content = f.read()
                    frontmatter, _, _ = self.parse_frontmatter(content)
                    if frontmatter and 'pub_date' in frontmatter:
                        pub_date = str(frontmatter['pub_date'])
                        year = int(pub_date.split('-')[0])
                        if year >= year_filter:
                            filtered_files.append(filepath)
                except:
                    continue
            md_files = filtered_files

        print(f"\n{'='*60}")
        print(f"Frontmatter ìë™ ê°œì„ ")
        print(f"{'='*60}")
        print(f"ëŒ€ìƒ ë””ë ‰í† ë¦¬: {directory}")
        print(f"ë§ˆí¬ë‹¤ìš´ íŒŒì¼: {len(md_files)}ê°œ")
        print(f"ë…„ë„ í•„í„°: {year_filter}ë…„ ì´í›„")
        if max_files:
            print(f"ìµœëŒ€ ì²˜ë¦¬ íŒŒì¼: {max_files}ê°œ")
        if self.dry_run:
            print(f"âš ï¸  DRY RUN ëª¨ë“œ (ì‹¤ì œ ì €ì¥ ì•ˆí•¨)")
        print(f"{'='*60}\n")

        improved = 0
        skipped = 0
        failed = 0

        for i, filepath in enumerate(md_files, 1):
            if max_files and improved >= max_files:
                print(f"\nâ¹ï¸  ìµœëŒ€ ì²˜ë¦¬ íŒŒì¼ ìˆ˜({max_files})ì— ë„ë‹¬")
                break

            result = self.improve_frontmatter(filepath)

            if result is True:
                improved += 1
            elif result is False:
                skipped += 1
            else:
                failed += 1

        # Summary
        print(f"\n{'='*60}")
        print(f"ê°œì„  ì™„ë£Œ")
        print(f"{'='*60}")
        print(f"âœ… ê°œì„  ì™„ë£Œ: {improved}ê°œ")
        print(f"â­ï¸  ê±´ë„ˆë›´ íŒŒì¼: {skipped}ê°œ")
        if failed > 0:
            print(f"âŒ ì‹¤íŒ¨: {failed}ê°œ")
        print(f"{'='*60}\n")


def main():
    import argparse

    parser = argparse.ArgumentParser(description='Frontmatter ìë™ ê°œì„ ')
    parser.add_argument('--year', type=int, default=2020,
                       help='ì´ ë…„ë„ ì´í›„ íŒŒì¼ë§Œ ì²˜ë¦¬ (ê¸°ë³¸: 2020)')
    parser.add_argument('--max-files', type=int, default=None,
                       help='ìµœëŒ€ ì²˜ë¦¬ íŒŒì¼ ìˆ˜')
    parser.add_argument('--dry-run', action='store_true',
                       help='ì‹¤ì œ ë³€í™˜í•˜ì§€ ì•Šê³  í…ŒìŠ¤íŠ¸ë§Œ')
    parser.add_argument('--directory', type=str, default='contents',
                       help='ëŒ€ìƒ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: contents)')

    args = parser.parse_args()

    try:
        import yaml
    except ImportError:
        print("âŒ PyYAML not installed. Installing...")
        os.system("uv pip install pyyaml")

    improver = FrontmatterImprover(dry_run=args.dry_run)
    improver.improve_directory(
        directory=Path(args.directory),
        year_filter=args.year,
        max_files=args.max_files
    )


if __name__ == '__main__':
    main()
